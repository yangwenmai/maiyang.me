---
layout: post
title: '数据爬取基础'
keywords: 数据, spider, 基础
date: 2016-01-05 20:00
description: '数据爬取基础'
categories: [数据, 基础]
tags: [数据, 基础]
comments: true
group: archive
icon: file-o
---

### 背景 ###

如果大家有看马云近期在不同场合的演讲，肯定发现了一个高频词：IT与DT时代，我们现在已经是DT时代了。

	DT:Data Technology
	IT:Information technology

<!-- more -->


那重要的就是Data，而数据要怎么为我们所用呢？大家首先想到的，肯定是搜索引擎(`Google, Baidu，Yahoo等`)。

### 概念 ###

我们可以在上面找到我们所想要的任何数据，如果你目的性很强，可以到指定网站去查询。比方说全球最大的WiKi网站,[维基百科](https://www.wikipedia.org/)，国内也有各种百科，自己去搜索吧。

这些数据都是存在网络上的，如果我们想进行一些数据分析，那些只在网络上的数据是很难被处理的。

所以我们回到了今天的主题：**数据爬取**
> 数据爬取简而言之：通过一种数据下载手段将网络上的数据copy到本机。

数据爬取，一般也就是指的网页数据爬取,也被称为“网络蜘蛛”。

### 实战 ###

爬虫程序也被看做是最好的语言入门练手项目，Java,Python,Golang等等，都有很好的开源库。Python应该是这方面的鼻祖。

> Python: [pyspider](https://github.com/binux/pyspider)、[scrapy](https://github.com/scrapy/scrapy)

> Java: [webmagic](https://github.com/code4craft/webmagic)

> Golang: [pholcus](https://github.com/henrylee2cn/pholcus)、[go_spider](https://github.com/hu17889/go_spider)

### 分析 ###

爬虫程序的最最基础：HTTP协议

HTTP协议我不展开讲，只要满足基本的HTTP协议请求就能够得到正常的数据，剩下的就是解析HTML元素了。

这里主要提一个点: header，这个是爬虫程序最至关重要的。因为现在网站数据都会采取各种限制，以防止数据被恶意的爬取而被利用。特别是像淘宝、天猫、京东等电商类网站。

header里面可以设置以下key：

1. cookie
2. user-agent
3. accept-encoding
4. accept-language
5. referer
6. cache-control
	
其中最为关键的是

1. cookie
2. user-agent
3. referer

cookie，user-agent 自不必多说，网上随随便便一搜索就能讲的很清楚。

我单独把referer拿出来说一下，主要是在实战中发生因为未设置referer而导致URL请求无响应。URL（[淘宝商品XXX](https://detailskip.taobao.com/json/sib.htm?itemId=522116937611&sellerId=1670749266&prior=1&p=1&rcid=16&sts=336662528,1170936092631760900,72127962782138496,1157495477273396227&price=4800&vd=1&skil=false&st=1&pf=1&al=false&ap=0&ss=0&free=1&defaultCityId=110100&u=1&ct=1)），不管是通过浏览器，还是curl等访问，都不会得到任何结果。`返回状态码204 No Content`。

>HTTP来源地址（referer，或HTTP referer），是HTTP表头的一个字段，用来表示从哪儿链接到目前的网页，采用的格式是URL。换句话说，借着HTTP referer，目前的网页可以检查访客从哪里而来，这也常被用来对付伪造的跨网站请求。[来源地址](https://zh.wikipedia.org/wiki/HTTP%E5%8F%83%E7%85%A7%E4%BD%8D%E5%9D%80)

referer 被用于防盗链，比方说淘宝、七牛。

扩展阅读

>[七牛关于防盗链的阐述](http://kb.qiniu.com/52pw6cde)

>[微信加载资源失败可能原因](http://kb.qiniu.com/5senwpdr)

要解决上面的URL无法得到数据的问题，很简单（设置header信息，特别是cookie，referer），referer这个的防盗链设置跟服务器端的校验有关，不一定每个URL都需要设置的。

还需要注意的是在解析返回结果的时候，要通过gzip来解压，因为我们请求的时候在header里面设置了 `accept-encoding: gzip,deflate,sdch"

说了这么多，有点混乱，凑合着看吧。

### 总结 ###

HTTP协议是最最基础的内容，每个开发人员必须要熟悉，并且要不断的温故而知新。

### 小技巧 ###

chrome 小技巧，打开审查元素，然后切换至Network，当我们浏览访问URL的时候，这里会将网页请求资源逐一显示出来，我们可以针对自己需要的资源点击右键，然后会弹窗出来很多可选择的功能项，我要说的是copy as cURL，不妨告诉你们，我就是通过这个完整数据，知道上面URL访问失败的原因的。

希望今天的文章对大家能有所帮助。
